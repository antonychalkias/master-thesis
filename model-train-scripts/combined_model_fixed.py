#!/usr/bin/env python3
"""
Combined model for food recognition and weight estimation.
This file was automatically generated by combining model.py, data.py, utils.py, and train.py.
"""

import torch.nn as nn
from torchvision.models import resnet50, ResNet50_Weights
from torch.utils.data import Dataset, DataLoader, random_split
from PIL import Image
from torchvision import transforms
import torch
import os
import pandas as pd
import argparse
import json
import torch.optim as optim
import torch.multiprocessing as mp
from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR
from torch.optim import Adam
try:
    from one_cycle_lr import OneCycleLR as CustomOneCycleLR
except ImportError:
    CustomOneCycleLR = None
from sklearn.metrics import accuracy_score, mean_absolute_error
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np


# ========================= model.py =========================
"""
Model architecture for food recognition and weight estimation.
"""

from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights

class MultiTaskNet(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        # Load pretrained EfficientNet-B0
        self.backbone = efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)

        # Get the number of features before the classification layer
        num_features = self.backbone.classifier[1].in_features

        # Remove original classifier head
        self.backbone.classifier = nn.Identity()

        # Add your multitask heads
        self.classifier = nn.Linear(num_features, num_classes)
        self.regressor = nn.Linear(num_features, 1)

    def forward(self, x):
        features = self.backbone(x)
        class_logits = self.classifier(features)
        weight_pred = self.regressor(features).squeeze(1)
        return class_logits, weight_pred


# ========================= data.py =========================
"""
Data processing and loading for food recognition and weight estimation model.
"""

from torch.utils.data import Dataset, DataLoader, random_split
from PIL import Image
from torchvision import transforms
import torch
import os
import pandas as pd

class FoodDataset(Dataset):
    def __init__(self, dataframe, image_dir, transform=None):
        self.df = dataframe.reset_index(drop=True)
        self.image_dir = image_dir
        self.transform = transform if transform else transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
            transforms.RandomHorizontalFlip(),
            transforms.RandomRotation(15),
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                std=[0.229, 0.224, 0.225])
        ])

        # Cache for found paths to speed up loading
        self.path_cache = {}
    
    def __len__(self):
        return len(self.df)
    
    def _try_load_image(self, path, row):
        """Helper to attempt loading an image from a path"""
        try:
            image = Image.open(path).convert('RGB')
            image = self.transform(image)
            return image, torch.tensor(row['label_idx'], dtype=torch.long), torch.tensor(row['weight'], dtype=torch.float32)
        except Exception:
            return None
    
    def _create_placeholder(self, row):
        """Create a placeholder black image for missing files"""
        image = Image.new('RGB', (224, 224), color='black')
        image = self.transform(image)
        return image, torch.tensor(row['label_idx'], dtype=torch.long), torch.tensor(row['weight'], dtype=torch.float32)
    
    def _find_image_path(self, img_name):
        """Find the correct path for an image, handling different cases and extensions"""
        # Get base name without extension
        name_without_ext, _ = os.path.splitext(img_name)
        
        # 1. Try original path first
        original_path = os.path.join(self.image_dir, img_name)
        if os.path.exists(original_path):
            return original_path
            
        # 2. Try with common extensions
        for ext in ['.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG']:
            test_path = os.path.join(self.image_dir, name_without_ext + ext)
            if os.path.exists(test_path):
                return test_path
        
        # 3. Case-insensitive search
        try:
            name_lower = name_without_ext.lower()
            for file in os.listdir(self.image_dir):
                file_name, _ = os.path.splitext(file)
                if file_name.lower() == name_lower:
                    return os.path.join(self.image_dir, file)
        except Exception as e:
            print(f"Error during file search: {e}")
            
        # Not found
        return None
    
    def __getitem__(self, idx):
        try:
            row = self.df.iloc[idx]
            img_name = row['image_name']
            
            # Check cache first
            if img_name in self.path_cache:
                cached_path = self.path_cache[img_name]
                if cached_path == "PLACEHOLDER":
                    return self._create_placeholder(row)
                    
                result = self._try_load_image(cached_path, row)
                if result is not None:
                    return result
                # Path no longer valid, clear from cache
                del self.path_cache[img_name]
            
            # Try to find the image path
            img_path = self._find_image_path(img_name)
            
            if img_path:
                # Found a path, try to load it
                result = self._try_load_image(img_path, row)
                if result is not None:
                    self.path_cache[img_name] = img_path
                    return result
            
            # If we get here, image wasn't found or couldn't be loaded
            print(f"Warning: Image {img_name} not found or corrupted, using placeholder")
            self.path_cache[img_name] = "PLACEHOLDER"
            return self._create_placeholder(row)
            
        except Exception as e:
            print(f"Unexpected error for index {idx}: {e}")
            return self._create_placeholder(row)

def prepare_data(csv_path, images_dir, batch_size=16, num_workers=0):
    """
    Prepare data for training and validation
    """
    # Load and process CSV
    df = pd.read_csv(csv_path, sep=';', quotechar='"')
    print(f"Successfully loaded {len(df)} records from {csv_path}")

    # Create label-to-index mapping
    label_to_idx = {label: idx for idx, label in enumerate(df['labels'].unique())}
    df['label_idx'] = df['labels'].map(label_to_idx)

    print(f"Number of classes: {len(label_to_idx)}")
    print(df.head())

    # Define train and val transforms separately
    train_transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(15),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225])
    ])

    val_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225])
    ])

    # Create base dataset with dummy transform (we'll override it below)
    base_dataset = FoodDataset(df, images_dir, transform=None)

    # Split into train/val
    train_size = int(0.8 * len(base_dataset))
    val_size = len(base_dataset) - train_size
    train_dataset, val_dataset = random_split(base_dataset, [train_size, val_size])

    # Assign correct transforms
    train_dataset.dataset.transform = train_transform
    val_dataset.dataset.transform = val_transform

    # DataLoaders
    train_dataloader = DataLoader(
        train_dataset, batch_size=batch_size, shuffle=True,
        num_workers=num_workers, pin_memory=True
    )

    val_dataloader = DataLoader(
        val_dataset, batch_size=batch_size, shuffle=False,
        num_workers=num_workers, pin_memory=True
    )

    print(f"Training on {train_size} samples, validating on {val_size} samples")

    return train_dataloader, val_dataloader, label_to_idx



# ========================= utils.py =========================
"""
Utility functions for food recognition and weight estimation model.
"""

import os
import argparse

def parse_args():
    """
    Parse command-line arguments for the training script.
    
    Returns:
        args: Parsed arguments
    """
    parser = argparse.ArgumentParser(description="Train food recognition and weight estimation model")
    parser.add_argument("--csv_path", type=str, default=None, help="Path to the CSV file with annotations")
    parser.add_argument("--images_dir", type=str, default=None, help="Path to the directory with images")
    parser.add_argument("--epochs", type=int, default=20, help="Number of training epochs")
    parser.add_argument("--batch_size", type=int, default=16, help="Batch size for training")
    parser.add_argument("--lr", type=float, default=1e-4, help="Learning rate")
    parser.add_argument("--model_dir", type=str, default=None, help="Directory to save the model")
    parser.add_argument("--num_workers", type=int, default=0, help="Number of workers for data loading")
    parser.add_argument('--lr_strategy', type=str, default='one_cycle', choices=['one_cycle', 'cosine', 'step'], help='Learning rate scheduler strategy')
    return parser.parse_args()

def setup_paths(args):
    """
    Set up paths for CSV file, images directory, and model save directory.
    
    Args:
        args: Parsed command-line arguments
        
    Returns:
        args: Updated arguments with default paths set
    """
    # Get project root directory
    master_thesis_dir = "/kaggle/working"
    
    # Set default paths if not provided
    if args.csv_path is None:
        args.csv_path = os.path.join(master_thesis_dir, "csvfiles", "labels.csv")
        # Check if file exists, if not try with the small test file
        if not os.path.exists(args.csv_path):
            print(f"Warning: CSV file not found at {args.csv_path}")
            # Try alternative path
            alternative_csv = os.path.join(master_thesis_dir, "csvfiles", "labels_latest_with_4_rows.csv")
            if os.path.exists(alternative_csv):
                print(f"Using alternative CSV file: {alternative_csv}")
                args.csv_path = alternative_csv
    
    if args.images_dir is None:
        args.images_dir = os.path.join(master_thesis_dir, "images")
    
    if args.model_dir is None:
        args.model_dir = os.path.join(master_thesis_dir, "models")
    
    print(f"CSV path: {args.csv_path}")
    print(f"Images directory: {args.images_dir}")
    print(f"Model save directory: {args.model_dir}")
    
    # Create model save directory
    os.makedirs(args.model_dir, exist_ok=True)
    
    return args

def get_device():
    if torch.cuda.is_available():
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")
    print(f"Using device: {device}")
    return device



# ========================= train.py =========================
"""
Training script for food recognition and weight estimation model.
"""

import os
import json
import torch
import torch.nn as nn
import torch.optim as optim
import torch.multiprocessing as mp
from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR
from torch.optim import Adam
try:
    from one_cycle_lr import OneCycleLR as CustomOneCycleLR
except ImportError:
    CustomOneCycleLR = None
from sklearn.metrics import accuracy_score, mean_absolute_error

def get_scheduler(optimizer, lr_strategy, num_epochs, steps_per_epoch, lr, min_lr=1e-6):
    if lr_strategy == 'one_cycle':
        if CustomOneCycleLR is not None:
            return CustomOneCycleLR(
                optimizer,
                max_lr=lr,
                total_epochs=num_epochs,
                steps_per_epoch=steps_per_epoch
            )
        else:
            return torch.optim.lr_scheduler.OneCycleLR(
                optimizer,
                max_lr=lr,
                steps_per_epoch=steps_per_epoch,
                epochs=num_epochs
            )
    elif lr_strategy == 'cosine':
        return CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=min_lr)
    elif lr_strategy == 'step':
        return StepLR(optimizer, step_size=5, gamma=0.75)
    else:
        return None

def _train_one_epoch(model, dataloader, optimizer, scheduler, device, criterion_class, criterion_weight, lr_strategy):
    model.train()
    running_loss = 0.0
    for images, labels, weights in dataloader:
        images = images.to(device, non_blocking=True)
        labels = labels.to(device, non_blocking=True)
        weights = weights.to(device, non_blocking=True)
        optimizer.zero_grad()
        outputs_class, outputs_weight = model(images)
        loss_class = criterion_class(outputs_class, labels)
        loss_weight = criterion_weight(outputs_weight, weights)
        total_loss = 0.9 * loss_class + 0.1 * loss_weight
        total_loss.backward()
        optimizer.step()
        if scheduler is not None and lr_strategy == 'one_cycle':
            scheduler.step()
        running_loss += total_loss.item()
    return running_loss / len(dataloader)

def _validate_one_epoch(model, dataloader, device, criterion_class, criterion_weight):
    model.eval()
    running_loss = 0.0
    all_preds, all_labels = [], []
    all_weight_preds, all_weight_true = [], []
    with torch.no_grad():
        for images, labels, weights in dataloader:
            images = images.to(device, non_blocking=True)
            labels = labels.to(device, non_blocking=True)
            weights = weights.to(device, non_blocking=True)
            outputs_class, outputs_weight = model(images)
            loss_class = criterion_class(outputs_class, labels)
            loss_weight = criterion_weight(outputs_weight, weights)
            total_loss = loss_class + loss_weight
            running_loss += total_loss.item()
            _, predicted = torch.max(outputs_class, 1)
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_weight_preds.extend(outputs_weight.cpu().numpy())
            all_weight_true.extend(weights.cpu().numpy())
    avg_loss = running_loss / len(dataloader)
    accuracy = accuracy_score(all_labels, all_preds)
    mae = mean_absolute_error(all_weight_true, all_weight_preds)
    return avg_loss, accuracy, mae

def _save_best_model(model, optimizer, avg_val_loss, val_accuracy, val_mae, label_to_idx, model_save_dir, epoch):
    model_path = os.path.join(model_save_dir, "best_model.pth")
    save_model = False
    prev_val_accuracy = 0
    prev_val_loss = float('inf')
    prev_val_mae = float('inf')
    if os.path.exists(model_path):
        try:
            prev_ckpt = torch.load(model_path, map_location='cpu', weights_only=False)
            prev_val_accuracy = prev_ckpt.get('val_accuracy', 0)
            prev_val_loss = prev_ckpt.get('val_loss', float('inf'))
            prev_val_mae = prev_ckpt.get('val_mae', float('inf'))
            if epoch == 0:
                print(f"Found existing model with val_loss={prev_val_loss:.4f}, val_accuracy={prev_val_accuracy:.4f}, val_mae={prev_val_mae:.2f}g")
            
            # Composite score: Higher accuracy is better, lower MAE is better
            prev_score = prev_val_accuracy - (prev_val_mae / 100)  # Scale MAE to be comparable to accuracy
            current_score = val_accuracy - (val_mae / 100)
            
            if current_score > prev_score:
                save_model = True
                print(f"Better combined score: {current_score:.4f} vs {prev_score:.4f}")
            else:
                print(f"Worse combined score: {current_score:.4f} vs {prev_score:.4f} - not saving")
        except Exception as e:
            print(f"Warning: Could not load previous checkpoint for comparison: {e}")
            save_model = True
    else:
        save_model = True
        print(f"No previous model found, saving first model")
    
    if save_model:
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'val_loss': avg_val_loss,
            'val_accuracy': val_accuracy,
            'val_mae': val_mae,
            'label_to_idx': label_to_idx
        }, model_path)
        print(f"Model saved to {model_path}")
    
    return save_model

def train_model(model, train_dataloader, val_dataloader, device, num_epochs, model_save_dir, lr_strategy='one_cycle', lr=1e-4):
    """
    Train the model and save the best checkpoint
    
    Args:
        model: Model to train
        train_dataloader: DataLoader for training data
        val_dataloader: DataLoader for validation data
        device: Device to use for training
        num_epochs: Number of epochs to train for
        model_save_dir: Directory to save the model
        lr_strategy: Learning rate strategy ('one_cycle', 'cosine', 'step')
        lr: Learning rate
    
    Returns:
        training_logs: Dictionary containing training metrics
    """
    # Loss functions
    criterion_class = nn.CrossEntropyLoss()
    criterion_weight = nn.MSELoss()
    
    # Optimizer
    optimizer = Adam(model.parameters(), lr=lr, weight_decay=1e-5)
    steps_per_epoch = len(train_dataloader)
    scheduler = get_scheduler(optimizer, lr_strategy, num_epochs, steps_per_epoch, lr)
    print(f"Using device: {device}")
    print(f"Optimizer: Adam, lr={lr}")
    print(f"Scheduler: {lr_strategy}")
    print(f"Starting training for {num_epochs} epochs...")

    # Metrics tracking
    training_logs = {
        "epochs": [],
        "train_loss": [],
        "val_loss": [],
        "val_accuracy": [],
        "weight_mae": []
    }

    for epoch in range(num_epochs):
        avg_train_loss = _train_one_epoch(model, train_dataloader, optimizer, scheduler, device, criterion_class, criterion_weight, lr_strategy)
        avg_val_loss, val_accuracy, val_mae = _validate_one_epoch(model, val_dataloader, device, criterion_class, criterion_weight)
        print(f"Epoch {epoch+1}/{num_epochs}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}, "
              f"Val Acc = {val_accuracy:.4f}, Weight MAE = {val_mae:.2f}g")
        
        # Save epoch metrics to log
        training_logs["epochs"].append(epoch + 1)
        training_logs["train_loss"].append(avg_train_loss)
        training_logs["val_loss"].append(avg_val_loss)
        training_logs["val_accuracy"].append(val_accuracy)
        training_logs["weight_mae"].append(val_mae)


        # Safely extract label mapping
        label_df = getattr(train_dataloader.dataset.dataset, 'df', None)
        if label_df is not None:
            label_to_idx = {label: idx for idx, label in enumerate(label_df['labels'].unique())}
        else:
            label_to_idx = {}
        
        _save_best_model(
            model,
            optimizer,
            avg_val_loss,
            val_accuracy,
            val_mae,
            label_to_idx,
            model_save_dir,
            epoch
        )

        if scheduler is not None and lr_strategy != 'one_cycle':
            scheduler.step()

    return training_logs


def convert_logs_to_json_safe(logs):
    def to_json_safe(x):
        if isinstance(x, torch.Tensor):
            return x.item() if x.numel() == 1 else x.tolist()
        elif isinstance(x, (np.generic,)):  # Covers all numpy scalar types
            return x.item()
        elif isinstance(x, (float, int)):
            return x
        else:
            return x

    return {k: [to_json_safe(val) for val in v] for k, v in logs.items()}

if __name__ == '__main__':
    # Required for multiprocessing on macOS
    # mp.set_start_method('spawn', force=True)
    
    # Define paths inside Kaggle
    DATA_PATH = "/kaggle/input/data-set-labeld-weights"
    args = argparse.Namespace(
        csv_path=f"{DATA_PATH}/labels.csv",
        images_dir=f"{DATA_PATH}/image_set_2",
        model_dir="/kaggle/working",  # This is where Kaggle lets you write files
        epochs= 30,
        batch_size=8,
        lr=1e-4,
        num_workers=2,
        lr_strategy="one_cycle"  # Added to fix AttributeError
    )

    
    # Prepare data
    train_dataloader, val_dataloader, label_to_idx = prepare_data(
        args.csv_path, 
        args.images_dir, 
        batch_size=args.batch_size, 
        num_workers=args.num_workers
    )
    
    # Get device (CUDA, MPS, or CPU)
    device = get_device()
    
    # Initialize model
    num_classes = len(label_to_idx)
    model = MultiTaskNet(num_classes)
    model.to(device)
    
    # Train the model
    training_logs = train_model(
        model, 
        train_dataloader, 
        val_dataloader, 
        device, 
        args.epochs, 
        args.model_dir,
        lr_strategy=args.lr_strategy,
        lr=args.lr
    )
    
    # Auto-increment JSON filename
    i = 1
    while True:
        log_path = os.path.join(args.model_dir, f"training_log_{i}.json")
        if not os.path.exists(log_path):
            break
        i += 1

    # Save training log
    safe_logs = convert_logs_to_json_safe(training_logs)
    with open(log_path, "w") as f:
        json.dump(safe_logs, f, indent=4)
